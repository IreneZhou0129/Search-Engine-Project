{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import string\n",
    "import json \n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from pythonds.basic.stack import Stack\n",
    "\n",
    "#### Module 1\n",
    "### Connect to the CSI courses website\n",
    "url = 'https://catalogue.uottawa.ca/en/courses/csi/'\n",
    "response = urllib.request.urlopen(url)\n",
    "soup_packetpage = BeautifulSoup(response, 'lxml')\n",
    "items = soup_packetpage.findAll(\"div\", class_ = \"courseblock\")\n",
    "\n",
    "lemmat = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#### Module 3\n",
    "def preprocessing(str_info):\n",
    "    tokenized_word = word_tokenize(str_info)\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    porter_stemmer = PorterStemmer()\n",
    "\n",
    "    sp_removed_word = []\n",
    "    # Stopword Removal\n",
    "    stop_words = set(stopwords.words('english')\\\n",
    "                     +stopwords.words('french')\\\n",
    "                     +list(string.ascii_lowercase)\\\n",
    "                     +[ str(i) for i in range(0,10)])\n",
    "    for word in tokenized_word:\n",
    "        # Lemmatization\n",
    "        word = wordnet_lemmatizer.lemmatize(word)\n",
    "    \n",
    "        # Stemming - Porter Stemmer \n",
    "        word = porter_stemmer.stem(word)\n",
    "    \n",
    "        if word.lower() not in stop_words:\n",
    "            sp_removed_word.append(\n",
    "                word.lower())\n",
    "    \n",
    "    return sp_removed_word\n",
    "\n",
    "\n",
    "\n",
    "def corpus_preprocessing(items):\n",
    "    '''input a list of result.Tags and return a list of dictionarized courses'''\n",
    "    # 1. Save the document ID, name(e.g., Database I) \n",
    "    # and the corresponding description into a dict - course:\n",
    "\n",
    "    # 2. Save all courses info into a list of dict - courses\n",
    "    courses = [] \n",
    "    docID = 0\n",
    "    for tag in items:\n",
    "        # Eliminate French Courses and the unit info\n",
    "        regex = r'(?P<code>\\w{3}\\xa0\\d{1}[1|2|3|4|9|0]\\d{2})\\s(?P<name>.*)'\n",
    "        \n",
    "        course = re.match(regex ,tag.find('strong').text.lower()) \n",
    "        if course:\n",
    "            course = course.groupdict()\n",
    "            course['docID'] = docID\n",
    "            course['name'] = re.sub(r'\\([^)]*\\)', '', course['name'])\n",
    "            course['description'] = tag.find('p',{'class':'courseblockdesc noindent'})\n",
    "            if course['description']:\n",
    "                course['description'] = course['description'].text\n",
    "                courses.append(course)\n",
    "            docID += 1\n",
    "    return courses\n",
    "# test case:\n",
    "# courses is a list of dict\n",
    "# keys: docID, name, and description\n",
    "courses = corpus_preprocessing(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 'csi\\xa01306',\n",
       " 'name': 'computing concepts for business ',\n",
       " 'docID': 0,\n",
       " 'description': '\\nIntroduction to computer-based problem solving from the perspective of the business world. Design of algorithms for solving business problems. Basics of computer programming in a modern programming language. Solving business problems using application packages including spreadsheets and databases. Basics of web design. Collaborative tools. Using open source software.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the list of dictionaries into json file\n",
    "import json\n",
    "with open('course_dic_lst','w') as fOut:\n",
    "    json.dump(courses, fOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Introduction to computer-based problem solving from the perspective of the business world. Design of algorithms for solving business problems. Basics of computer programming in a modern programming language. Solving business problems using application packages including spreadsheets and databases. Basics of web design. Collaborative tools. Using open source software.\n"
     ]
    }
   ],
   "source": [
    "# Read the data stored in the json file\n",
    "with open('course_dic_lst') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for p in data:\n",
    "        print(p['description'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def four_step_processing(curr):\n",
    "    # Tokenization \n",
    "    #   Concatenate course name and the description into one string\n",
    "    str_name = curr.get(\"name\")\n",
    "    str_description = curr.get(\"description\")\n",
    "    str_info = str_name + str_description\n",
    "\n",
    "    #   Remove the punctuation from the string\n",
    "    for c in string.punctuation:\n",
    "        str_info = str_info.replace(c,\"\")\n",
    "    return preprocessing(str_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_occurred_word(courses):\n",
    "    list_of_word_lists = []\n",
    "    for i in courses:\n",
    "        list_of_word_lists.append(four_step_processing(i))\n",
    "    \n",
    "    # Combine all sublists into one list\n",
    "    return list_of_word_lists ,sum(list_of_word_lists,[])\n",
    "\n",
    "list_of_word_lists ,one_list_word = all_occurred_word(courses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comput',\n",
       " 'concept',\n",
       " 'busi',\n",
       " 'introduct',\n",
       " 'computerbas',\n",
       " 'problem',\n",
       " 'solv',\n",
       " 'perspect',\n",
       " 'busi',\n",
       " 'world']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_list_word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary_building(courses):\n",
    "    # Indexing the word\n",
    "    #   Create the transform\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    lw, one_list_word = all_occurred_word(courses)\n",
    "    \n",
    "    #   Tokenize and build vocab\n",
    "    bow = vectorizer.fit_transform(one_list_word) \n",
    "    #   Summarize \n",
    "    indexed_word_dict = vectorizer.vocabulary_\n",
    "    print(\"feature names: \",vectorizer.get_feature_names())\n",
    "    print(\"idf: \",vectorizer.idf_)\n",
    "    return indexed_word_dict\n",
    "# test case:\n",
    "# indexed_word_dict = dictionary_building(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comput': 233, 'concept': 236}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dict(list(indexed_word_dict.items())[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexed_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional Module\n",
    "# Spelling correction: edit distance\n",
    "# ref: https://bit.ly/2T37dTt \n",
    "def compute_dist(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "def levenshteinDistance(s1):\n",
    "    # find the most similar word to one_list_word\n",
    "    min_dist = 1000\n",
    "    closest = None\n",
    "    if s1 not in one_list_word:\n",
    "        for s2 in one_list_word:\n",
    "            dist = compute_dist(s1, s2)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest = s2\n",
    "    else:\n",
    "        closest = None\n",
    "        min_dist = -1\n",
    "\n",
    "    return min_dist , closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Module 4\n",
    "# arguments: courses - document collection; indexed_word_dict - dictionary\n",
    "def inverted_index_construction(courses,indexed_word_dict):\n",
    "    docs = [four_step_processing(i) for i in courses]\n",
    "    inv_indx = defaultdict(list)\n",
    "    for idx, text in enumerate(docs):\n",
    "        for word in text:\n",
    "            inv_indx[word].append(idx)\n",
    "\n",
    "    result = {}\n",
    "    for key, val in inv_indx.items():\n",
    "        d = defaultdict(int)\n",
    "        for i in val:\n",
    "            d[i] += 1\n",
    "        result[key] = d.items()\n",
    "    \n",
    "    #result format: 'word':dic_items([(docID, weight), (docID, weight), ...])\n",
    "    return result\n",
    "# test case:\n",
    "# term_doc_ids = inverted_index_construction(courses,indexed_word_dict)\n",
    "\n",
    "#### Module 5\n",
    "def corpus_access(input_docIDs):\n",
    "    output_docs = []\n",
    "    length = len(input_docIDs)\n",
    "    for i in range(length):\n",
    "        curr_docID = input_docIDs.pop()\n",
    "        output_docs.append(courses[i])\n",
    "    return output_docs\n",
    "\n",
    "#### Module 6    \n",
    "def isOperator(word):\n",
    "    if(word.upper() in ['AND', 'OR', 'AND_NOT', '(', ')']):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def query_processing(user_query):\n",
    "    '''Query processing: 1. transform infix format to postfix format;\n",
    "                            ref: https://bit.ly/28OMA5X \n",
    "                         2. deal with the wildcard '''\n",
    "    # Split the query into a list of word, including the parentheses\n",
    "    pattern = re.compile(r\"([.()!])\")\n",
    "    tmp = (pattern.sub(\" \\\\1 \", user_query)).split(\" \")\n",
    "    tokenList = list(filter(None,tmp))\n",
    "    \n",
    "    # Transform infix to postfix \n",
    "    opStack = Stack()\n",
    "    postfixList = []\n",
    "    prec = {}\n",
    "    prec[\"(\"] = 1\n",
    "    prec[\"AND\"] = 2\n",
    "    prec[\"OR\"] = 2\n",
    "    prec[\"AND_NOT\"] = 2\n",
    "    for t in tokenList:\n",
    "        if (not(isOperator(t))):\n",
    "            postfixList.append(t)\n",
    "        elif t == \"(\":\n",
    "            opStack.push(t)\n",
    "        elif t == \")\":\n",
    "            topToken = opStack.pop()\n",
    "            while topToken != \"(\":\n",
    "                postfixList.append(topToken)\n",
    "                topToken = opStack.pop()\n",
    "        else:\n",
    "            while (not opStack.isEmpty()) and  (prec[opStack.peek()] >= prec[t]):\n",
    "                postfixList.append(opStack.pop())\n",
    "            opStack.push(t)\n",
    "\n",
    "    while (not opStack.isEmpty()):\n",
    "        postfixList.append(opStack.pop())\n",
    "    return \" \".join(postfixList)\n",
    "\n",
    "# test case:\n",
    "# user_query = \"algorithm AND (data OR structure) OR (comp AND ca)\"\n",
    "# user_query = \"perspective AND business\"\n",
    "\n",
    "# postfixQuery = query_processing(user_query)\n",
    "# postfixQuery1 = query_processing(user_query1)\n",
    "# print(postfixQuery1)\n",
    "\n",
    "# print(postfixQuery):\n",
    "# algorithm data structure OR AND comp prog AND OR\n",
    "# [\"algorithm\", \"data\", \"structure\", \"OR\", \"AND  \"comp\", \"prog\", \"AND\", \"OR\"]\n",
    "\n",
    "def get_term_docs_list(term_doc_ids,term):\n",
    "    term_docs_result = []\n",
    "    \n",
    "    # getting all of the doc_id list of tuple\n",
    "    if term not in term_doc_ids.keys():\n",
    "        term = 0\n",
    "    else:\n",
    "        # if the given term is incorrect \n",
    "        pass        \n",
    "    term_docs = term_doc_ids.get(term)\n",
    "    for k , v in term_docs:\n",
    "        term_docs_result.append(k)\n",
    "    return term_docs_result\n",
    "\n",
    "\n",
    "# Given dict_item-type args, return two lists\n",
    "def two_terms_docs_lists(term_doc_ids, term1, term2):\n",
    "    term1_docs_list = []\n",
    "    term2_docs_list = []\n",
    "    if(type(term1)!=list and type(term2)!=list):\n",
    "        term1_docs_list = get_term_docs_list(term_doc_ids, term1)\n",
    "        term2_docs_list = get_term_docs_list(term_doc_ids, term2)\n",
    "        \n",
    "    elif(type(term1)!=list):\n",
    "        term1_docs_list = get_term_docs_list(term_doc_ids, term1)\n",
    "\n",
    "    elif(type(term2)!=list):\n",
    "        term2_docs_list = get_term_docs_list(term_doc_ids, term2)\n",
    "        \n",
    "    else: # both of them are lists\n",
    "        term1_docs_list = term1\n",
    "        term2_docs_list = term2\n",
    "\n",
    "    return term1_docs_list, term2_docs_list\n",
    "    \n",
    "\n",
    "def and_op(term1_docs_list, term2_docs_list): \n",
    "    res = list(set(term1_docs_list) & set(term2_docs_list))\n",
    "    return res\n",
    "\n",
    "def or_op(term1_docs_list, term2_docs_list):\n",
    "    res = list(set(term1_docs_list) | set(term2_docs_list))\n",
    "    return res \n",
    "\n",
    "def not_op(term1_docs_list, term2_docs_list, last_dict):\n",
    "    last_doc_id = last_dict.get('docID')\n",
    "    all_docs = range(0, last_doc_id)\n",
    "    not_t2 = [x for x in all_docs if x not in term2_docs_list]\n",
    "    res = and_op(term1_docs_list, not_t2)\n",
    "    return res \n",
    "\n",
    "def find_list_of_terms(term_doc_ids):    \n",
    "    list_of_terms = []\n",
    "    for k, v in term_doc_ids.items():\n",
    "        list_of_terms.append(k)\n",
    "\n",
    "    return list_of_terms\n",
    "\n",
    "def boolean_retrieval(query, courses):\n",
    "    postfixQuery = query_processing(query)\n",
    " \n",
    "    indexed_word_dict = dictionary_building(courses)\n",
    "    term_doc_ids = inverted_index_construction(courses,indexed_word_dict)\n",
    "    \n",
    "    list_of_terms = find_list_of_terms(term_doc_ids)\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    edit_dist = None\n",
    "    # Turn the query into a list of word\n",
    "    word_list = postfixQuery.split(\" \")\n",
    "    tmp = []\n",
    "    for term in word_list:\n",
    "        if (not isOperator(term)):\n",
    "            # lemmatize\n",
    "            lemmatized_term = wordnet_lemmatizer.lemmatize(term)\n",
    "            # stemming \n",
    "            stemmed_term = porter_stemmer.stem(term)\n",
    "            \n",
    "            dist , fix_stemmed_term = levenshteinDistance(term)\n",
    "            \n",
    "            if fix_stemmed_term: \n",
    "                stemmed_term = fix_stemmed_term\n",
    "                edit_dist = (dist, fix_stemmed_term)\n",
    "            \n",
    "            stemmed_word_doc_ids = get_term_docs_list(term_doc_ids, stemmed_term)\n",
    "            \n",
    "            tmp.append(stemmed_word_doc_ids)\n",
    "                \n",
    "        elif(isOperator(term)):\n",
    "            t1 = tmp.pop()\n",
    "            t2 = tmp.pop()\n",
    "            l1, l2 = two_terms_docs_lists(term_doc_ids,t1,t2)\n",
    "            if (term == \"AND\"):\n",
    "                curr = and_op(l1, l2)\n",
    "            elif(term == \"OR\"):\n",
    "                curr = or_op(l1, l2)\n",
    "            elif(term == \"AND_NOT\"):\n",
    "                last_dict = courses[-1]\n",
    "                curr = and_not_op(t1, t2, last_dict)\n",
    "            tmp.append(curr)\n",
    "    \n",
    "    # ======================\n",
    "    #  compute edit_dist \n",
    "    # ======================\n",
    "    if edit_dist:\n",
    "        origin_query = \"\"\n",
    "        for token in word_list:\n",
    "            dist = compute_dist(token,edit_dist[1])\n",
    "            if dist <= edit_dist[0]:\n",
    "                origin_query += edit_dist[1] + \" \"\n",
    "            else:\n",
    "                origin_query += token + \" \"\n",
    "        \n",
    "        edit_dist = origin_query\n",
    "        \n",
    "    \n",
    "    return reduce((lambda x , y: or_op(x,y)),tmp) , edit_dist\n",
    "\n",
    "#### Module 7\n",
    "def vsm(query, courses):\n",
    "    #     add:\n",
    "    list_of_word_lists,one_list_word = all_occurred_word(courses)\n",
    "    \n",
    "    # Find idf for all word in the collection of docs\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    bow = vectorizer.fit_transform(one_list_word) \n",
    "    \n",
    "    size_of_words = len(vectorizer.vocabulary_.keys())\n",
    "    matrix = np.zeros((len(courses), size_of_words))\n",
    "    \n",
    "\n",
    "    \n",
    "    for document_index, words in enumerate(list_of_word_lists):\n",
    "        for word in words:\n",
    "            matrix[document_index, \n",
    "                    vectorizer.vocabulary_[ word.lower() ] ] += 1\n",
    "\n",
    "    word_idf = vectorizer.idf_\n",
    "    tf_idf = matrix * word_idf\n",
    "\n",
    "    query_vector = np.zeros((size_of_words))\n",
    "    query = preprocessing(query)\n",
    "    \n",
    "    edit_dist = None \n",
    "\n",
    "    for word in query:\n",
    "        if word not in vectorizer.vocabulary_.keys():\n",
    "            edit_dist = levenshteinDistance(word)\n",
    "            query_vector[vectorizer.vocabulary_[edit_dist[1]]] += 1\n",
    "        else:\n",
    "            query_vector[vectorizer.vocabulary_[word]] += 1\n",
    "    \n",
    "    \n",
    "    # ======================\n",
    "    #  compute edit_dist \n",
    "    # ======================\n",
    "    if edit_dist:\n",
    "        origin_query = \"\"\n",
    "        for token in query:\n",
    "            dist = compute_dist(token,edit_dist[1])\n",
    "            if dist == edit_dist[0]:\n",
    "                origin_query += edit_dist[1] + \" \"\n",
    "            else:\n",
    "                origin_query += token + \" \"\n",
    "        \n",
    "        edit_dist = origin_query\n",
    "\n",
    "    query_vector =  query_vector.T * word_idf\n",
    "    rank = np.matmul(matrix , query_vector)\n",
    "    \n",
    "    return list(reversed( np.argsort(rank))) , rank , edit_dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test case: \n",
    "# tf = vsm(\"algorithm\", courses)\n",
    "\n",
    "def test():\n",
    "    indexed_word_dict = dictionary_building(courses)\n",
    "    term_doc_ids = inverted_index_construction(courses,indexed_word_dict)\n",
    "    \n",
    "    input_docIDs = [10, 11, 12]\n",
    "    output_docs = corpus_access(input_docIDs)\n",
    "    print(\"corpus complete\")\n",
    "\n",
    "    boolean_test_query = \"algorithm AND (data OR structure) OR (comp AND ca)\"\n",
    "    test_postfixQuery = boolean_retrieval(boolean_test_query, courses)\n",
    "    print(\"boolean complete\")\n",
    "\n",
    "    vsm_test_query = \"algorithm data\"\n",
    "    test_vsm = vsm(vsm_test_query, courses)\n",
    "    print(\"vsm complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
